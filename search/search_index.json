{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes and HTC on demand hands on Contacts Diego Ciangottini (diego.ciangottini<at>pg.infn.it) Acknowledgements / prior work Kubernetes official tutorial Kubernetes by examples Helm documentation Infrastructures for Big Data Processing (BDP2) , Davide Salomoni OpenFaaS workshop , copyright OpenFaaS Author(s) . Before starting Log into the dedicated machine (if want to reproduce it at home you can find the Ansible playbooks in this repo) Clone the current repo in your home directory: git clone https://github.com/DODAS-TS/HandsOn-INFN-2019.git && cd HandsOn-INFN-2019 set the environment variable to use the k8s config file pointing to the demo cluster export KUBECONFIG = /home/centos/form<your number>/kubeconfig.yml You should now be able to see the cluster nodes with: $ kubectl get node NAME STATUS ROLES AGE VERSION vnode-0.localdomain Ready master 2d15h v1.14.0 vnode-1.localdomain Ready <none> 2d15h v1.14.0 vnode-2.localdomain Ready <none> 2d15h v1.14.0 vnode-3.localdomain Ready <none> 2d15h v1.14.0 vnode-4.localdomain Ready <none> 2d15h v1.14.0 And to inspect one of them with: kubectl describe node vnode-1.localdomain If so, you are ready to start. Hands-on guide Kubernetes mini tutorial Overview Pods Debugging Services CVMFS HTCondor on demand: Deployment Templating FaaS: Setup FaaS framework Hands on FaaS triggers Managing storage events and FaaS EXTRAS k3s - Deploy kubernetes cluster in 1 min for development Ansible deployment of a webserver PaaS Orchestrator","title":"Home"},{"location":"#kubernetes-and-htc-on-demand-hands-on","text":"","title":"Kubernetes and HTC on demand hands on"},{"location":"#contacts","text":"Diego Ciangottini (diego.ciangottini<at>pg.infn.it)","title":"Contacts"},{"location":"#acknowledgements-prior-work","text":"Kubernetes official tutorial Kubernetes by examples Helm documentation Infrastructures for Big Data Processing (BDP2) , Davide Salomoni OpenFaaS workshop , copyright OpenFaaS Author(s) .","title":"Acknowledgements / prior work"},{"location":"#before-starting","text":"Log into the dedicated machine (if want to reproduce it at home you can find the Ansible playbooks in this repo) Clone the current repo in your home directory: git clone https://github.com/DODAS-TS/HandsOn-INFN-2019.git && cd HandsOn-INFN-2019 set the environment variable to use the k8s config file pointing to the demo cluster export KUBECONFIG = /home/centos/form<your number>/kubeconfig.yml You should now be able to see the cluster nodes with: $ kubectl get node NAME STATUS ROLES AGE VERSION vnode-0.localdomain Ready master 2d15h v1.14.0 vnode-1.localdomain Ready <none> 2d15h v1.14.0 vnode-2.localdomain Ready <none> 2d15h v1.14.0 vnode-3.localdomain Ready <none> 2d15h v1.14.0 vnode-4.localdomain Ready <none> 2d15h v1.14.0 And to inspect one of them with: kubectl describe node vnode-1.localdomain If so, you are ready to start.","title":"Before starting"},{"location":"#hands-on-guide","text":"Kubernetes mini tutorial Overview Pods Debugging Services CVMFS HTCondor on demand: Deployment Templating FaaS: Setup FaaS framework Hands on FaaS triggers Managing storage events and FaaS","title":"Hands-on guide"},{"location":"#extras","text":"k3s - Deploy kubernetes cluster in 1 min for development Ansible deployment of a webserver PaaS Orchestrator","title":"EXTRAS"},{"location":"extras/ansible/","text":"Before starting download the hands-on repository and move inside the directory as follow: git clone https://github.com/DODAS-TS/SOSC-2018.git cd SOSC-2018 Automation with Ansible What's Ansible Ansible is a software that consistently automatize the configuration management. The Ansible configurations are a set of minimal infrastracture descriptions that are easy to read and to mantain, reducing the amount of work needed to setup environment and softwares on data centers or even on a laptop. Ansible does not require any remote agents and delivers all modules to remote systems that execute tasks, as needed, to deploy the desired configuration. Ansible Galaxy also has over 4,000 community-provided roles that can be used by anyone and tailored to different environments. In this hands-on we are going to deploy a simple web server application as a first introduction to Ansible recipes. Hello word: install and deploy a webserver with Ansible Playbooks Playbooks are the Ansible building blocks. They describe the desired infrastracture with a sequence of states and checks, that will be automatically deployed at installation time. Ansible infact implements a \"state-driven\" paradigm, that does not indicate the exact chain of commands but instead check a sequential list of machine state . But let's do an exercise to understand better the basics. install the apache2 packages on localhost --- - hosts : localhost connection : local tasks : - name : Apache | Make sure the Apache packages are installed apt : name=apache2 update_cache=yes when : ansible_os_family == \"Debian\" - name : Apache | Make sure the Apache packages are installed yum : name=httpd when : ansible_os_family == \"RedHat\" start services after configuration customization --- - hosts : localhost connection : local tasks : - name : Configure apache 1/2 get_url : force : true url : https://raw.githubusercontent.com/DODAS-TS/SOSC-2018/master/templates/hands-on-1/apache-config/port.conf dest : /etc/apache2/ports.conf - name : Configure apache 2/2 get_url : force : true url : https://raw.githubusercontent.com/DODAS-TS/SOSC-2018/master/templates/hands-on-1/apache-config/000-default.conf dest : /etc/apache2/sites-enabled/000-default.conf - name : Start Apache service service : name=apache2 state=restarted when : ansible_os_family == \"Debian\" - name : Start Apache service service : name=httpd state=restarted when : ansible_os_family == \"RedHat\" Run the playbooks install the apache2 packages on localhost ansible-playbook templates/hands-on-1/ansible-role-install.yml start services after configuration customization ansible-playbook templates/hands-on-1/ansible-role-apache.yaml the apache default webpage should now being served at localhost:4880 N.B. with this simple configuration files we will be able to reproduce with a single command the current state on different machines. Of course this is a very simple software installation that is meant as demonstrator, but in principle can be much more complicated.","title":"Ansible deployment of a webserver"},{"location":"extras/ansible/#automation-with-ansible","text":"","title":"Automation with Ansible"},{"location":"extras/ansible/#whats-ansible","text":"Ansible is a software that consistently automatize the configuration management. The Ansible configurations are a set of minimal infrastracture descriptions that are easy to read and to mantain, reducing the amount of work needed to setup environment and softwares on data centers or even on a laptop. Ansible does not require any remote agents and delivers all modules to remote systems that execute tasks, as needed, to deploy the desired configuration. Ansible Galaxy also has over 4,000 community-provided roles that can be used by anyone and tailored to different environments. In this hands-on we are going to deploy a simple web server application as a first introduction to Ansible recipes.","title":"What's Ansible"},{"location":"extras/ansible/#hello-word-install-and-deploy-a-webserver-with-ansible","text":"","title":"Hello word: install and deploy a webserver with Ansible"},{"location":"extras/ansible/#playbooks","text":"Playbooks are the Ansible building blocks. They describe the desired infrastracture with a sequence of states and checks, that will be automatically deployed at installation time. Ansible infact implements a \"state-driven\" paradigm, that does not indicate the exact chain of commands but instead check a sequential list of machine state . But let's do an exercise to understand better the basics. install the apache2 packages on localhost --- - hosts : localhost connection : local tasks : - name : Apache | Make sure the Apache packages are installed apt : name=apache2 update_cache=yes when : ansible_os_family == \"Debian\" - name : Apache | Make sure the Apache packages are installed yum : name=httpd when : ansible_os_family == \"RedHat\" start services after configuration customization --- - hosts : localhost connection : local tasks : - name : Configure apache 1/2 get_url : force : true url : https://raw.githubusercontent.com/DODAS-TS/SOSC-2018/master/templates/hands-on-1/apache-config/port.conf dest : /etc/apache2/ports.conf - name : Configure apache 2/2 get_url : force : true url : https://raw.githubusercontent.com/DODAS-TS/SOSC-2018/master/templates/hands-on-1/apache-config/000-default.conf dest : /etc/apache2/sites-enabled/000-default.conf - name : Start Apache service service : name=apache2 state=restarted when : ansible_os_family == \"Debian\" - name : Start Apache service service : name=httpd state=restarted when : ansible_os_family == \"RedHat\"","title":"Playbooks"},{"location":"extras/ansible/#run-the-playbooks","text":"install the apache2 packages on localhost ansible-playbook templates/hands-on-1/ansible-role-install.yml start services after configuration customization ansible-playbook templates/hands-on-1/ansible-role-apache.yaml the apache default webpage should now being served at localhost:4880 N.B. with this simple configuration files we will be able to reproduce with a single command the current state on different machines. Of course this is a very simple software installation that is meant as demonstrator, but in principle can be much more complicated.","title":"Run the playbooks"},{"location":"extras/orchent/","text":"Introduction to cloud platforms When working with cloud resources, depending on the user needs, different layers of underlyng abstraction can be needed, and depending on how many layers and their composition one can define different categories. Platform as a Service on top of Infrastracture as a Service Infrastructure as a service (IaaS) is a cloud computing offering in which a vendor provides users access to computing resources such as servers , storage and networking. Organizations use their own platforms and applications within a service provider\u2019s infrastructure. Key features: Instead of purchasing hardware outright, users pay for IaaS on demand. Infrastructure is scalable depending on processing and storage needs. Saves enterprises the costs of buying and maintaining their own hardware. Because data is on the cloud, there can be no single point of failure. Enables the virtualization of administrative tasks, freeing up time for other work. Platform as a service (PaaS) is a cloud computing offering that provides users with a cloud environment in which they can develop, manage and deliver applications. In addition to storage and other computing resources, users are able to use a suite of prebuilt tools to develop, customize and test their own applications. Key features: SaaS vendors provide users with software and applications via a subscription model. Users do not have to manage, install or upgrade software; SaaS providers manage this. Data is secure in the cloud; equipment failure does not result in loss of data. Use of resources can be scaled depending on service needs. Applications are accessible from almost any internet-connected device, from virtually anywhere in the world. N.B. In this hands-on a simple VM will be deployed, as an example, on cloud resources in an automated way thanks the use of a PaaS orchestrator and TOSCA system description files. More complicated recipices can provide you with a working k8s cluster where you can setup a FaaS framework as you will use in the next chapters. INDIGO-DC PaaS orchestrator The INDIGO PaaS Orchestrator allows to instantiate resources on Cloud Management Frameworks (like OpenStack and OpenNebula) platforms based on deployment requests that are expressed through templates written in TOSCA YAML Simple Profile v1.0 , and deploys them on the best cloud site available. Requirement First of you need to register to the service as described here . N.B. please put in the registration note \"SOSC2019 student\". Requests without this note will not be accepted. Please also notice that the resources instantiated for the school will be removed from the test pool few days after the end of the school. Install deployment client sudo apt install -y jq unzip wget https://github.com/Cloud-PG/dodas-go-client/releases/download/v0.2.2/dodas.zip unzip dodas.zip sudo mv dodas /usr/local/bin/","title":"PaaS Orchestrator"},{"location":"extras/orchent/#introduction-to-cloud-platforms","text":"When working with cloud resources, depending on the user needs, different layers of underlyng abstraction can be needed, and depending on how many layers and their composition one can define different categories.","title":"Introduction to cloud platforms"},{"location":"extras/orchent/#platform-as-a-service-on-top-of-infrastracture-as-a-service","text":"Infrastructure as a service (IaaS) is a cloud computing offering in which a vendor provides users access to computing resources such as servers , storage and networking. Organizations use their own platforms and applications within a service provider\u2019s infrastructure. Key features: Instead of purchasing hardware outright, users pay for IaaS on demand. Infrastructure is scalable depending on processing and storage needs. Saves enterprises the costs of buying and maintaining their own hardware. Because data is on the cloud, there can be no single point of failure. Enables the virtualization of administrative tasks, freeing up time for other work. Platform as a service (PaaS) is a cloud computing offering that provides users with a cloud environment in which they can develop, manage and deliver applications. In addition to storage and other computing resources, users are able to use a suite of prebuilt tools to develop, customize and test their own applications. Key features: SaaS vendors provide users with software and applications via a subscription model. Users do not have to manage, install or upgrade software; SaaS providers manage this. Data is secure in the cloud; equipment failure does not result in loss of data. Use of resources can be scaled depending on service needs. Applications are accessible from almost any internet-connected device, from virtually anywhere in the world. N.B. In this hands-on a simple VM will be deployed, as an example, on cloud resources in an automated way thanks the use of a PaaS orchestrator and TOSCA system description files. More complicated recipices can provide you with a working k8s cluster where you can setup a FaaS framework as you will use in the next chapters.","title":"Platform as a Service on top of Infrastracture as a Service"},{"location":"extras/orchent/#indigo-dc-paas-orchestrator","text":"The INDIGO PaaS Orchestrator allows to instantiate resources on Cloud Management Frameworks (like OpenStack and OpenNebula) platforms based on deployment requests that are expressed through templates written in TOSCA YAML Simple Profile v1.0 , and deploys them on the best cloud site available.","title":"INDIGO-DC PaaS orchestrator"},{"location":"extras/orchent/#requirement","text":"First of you need to register to the service as described here . N.B. please put in the registration note \"SOSC2019 student\". Requests without this note will not be accepted. Please also notice that the resources instantiated for the school will be removed from the test pool few days after the end of the school.","title":"Requirement"},{"location":"extras/orchent/#install-deployment-client","text":"sudo apt install -y jq unzip wget https://github.com/Cloud-PG/dodas-go-client/releases/download/v0.2.2/dodas.zip unzip dodas.zip sudo mv dodas /usr/local/bin/","title":"Install deployment client"},{"location":"faas/events/","text":"Working with functions You can get a local environment ready using Vagrant for an automatically setting up a machine on Virtualbox. To download Vagrant follow this link where you can find the complete list based on Os System: https://www.vagrantup.com/downloads.html Vagrant installation - Windows Download the following file and install it: https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_x86_64.msi Vagrant installation - Linux Download the package and extract it just pasting the following commands: wget https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_linux_amd64.zip unzip vagrant_2.2.5_linux_amd64.zip You can also use Homebrew like this: brew cask install vagrant If you prefer, you can use your favorite package manager. For example, in Ubuntu you can type: sudo apt install virtualbox vagrant Vagrant installation - MacOS Download the dmg file and install it: https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_x86_64.dmg If you have Homebrew installed just type this in the command line: brew cask install vagrant Install Vagrant project To be operative with the current project you have to download with git or extract the zip file . Open your command line and paste the following commands: git clone https://github.com/Cloud-PG/SOSC2019.git cd SOSC2019 Now set up the vagrant environment using the following command: ./vagrant up # Or if you have vagrant executable in your PATH vagrant up NOTE : This may take few minutes, depending on network and your computer performance And then, log into the created machine: ./vagrant ssh # Or if you have vagrant executable in your PATH vagrant ssh Remember also to authenticate with OpenFaaS cli before continuing: export OPENFAAS_URL = http://127.0.0.1:31112 cat gateway-password.txt | faas-cli login --password-stdin NOTE : Vagrant and Virtualbox are required on the machine of course. If you don't have them check the previous steps to install the packages Some additional machines have been prepared for the school participants. You can find hot to access to your machine here Using example functions and the OpenFaaS UI Now you can go to http://localhost:31112/ui/ and, using the password in gateway_password.txt with user admin , you should be able to log in. To see your password just type: cat gateway-password.txt You will have a page like that as result after the login: Let's start playing with some example functions. For instance, you can instantiate a function the face-detection of an online image just clicking on Deploy new function , searching for opencv and installing face-detect with OpenCV (button Deploy ). Now a new tab should appear with the function name selected. From there you can check the status and also try to invoke the function from the UI. For instance, as soon as the status of the function is ready, let's try to put an url with a jpg image in the request body field and then press invoke. Let's try the two below for example: https://parismatch.be/app/uploads/2018/04/Macaca_nigra_self-portrait_large-e1524567086123-1100x715.jpg http://thedreamwithinpictures.com/wp-content/uploads/2013/05/c3a89__montage_2048-copy.jpg For the second one you will have the following result: The list of all available functions in the store is also available from CLI using the following command from the vagrant machine already created: faas-cli store list Deployment of a Python function (from OpenFaaS workshop ) Do everyone have a docker account? mkdir astronaut-finder cd astronaut-finder faas-cli new --lang python3 astronaut-finder --prefix = \"<your-docker-username-here>\" Function fundamentals The previous command will write three files for us: ./astronaut-finder/handler.py This is the handler for the function. NOTE : an handler get a request object with the raw request and can print the result of the function to the console. ./astronaut-finder/requirements.txt Use this file to list any pip modules you want to install so, to manage your Python requirements, such as requests or urllib ./astronaut-finder.yml This file is used to manage the function: NOTE : it has the name of the function, the Docker image and any other customizations needed. Edit ./astronaut-finder/requirements.txt and add the following dependency: requests Write the function's code We'll be pulling in data from http://api.open-notify.org/astros.json Here's an example of the result we will have from that url: { \"number\" : 6 , \"people\" : [ { \"craft\" : \"ISS\" , \"name\" : \"Alexander Misurkin\" }, { \"craft\" : \"ISS\" , \"name\" : \"Mark Vande Hei\" }, { \"craft\" : \"ISS\" , \"name\" : \"Joe Acaba\" }, { \"craft\" : \"ISS\" , \"name\" : \"Anton Shkaplerov\" }, { \"craft\" : \"ISS\" , \"name\" : \"Scott Tingle\" }, { \"craft\" : \"ISS\" , \"name\" : \"Norishige Kanai\" } ], \"message\" : \"success\" } Let's write an handler that gets for us that result. You have to edit handler.py : import requests import random def handle ( req ): r = requests . get ( \"http://api.open-notify.org/astros.json\" ) result = r . json () index = random . randint ( 0 , result [ \"number\" ] - 1 ) name = result [ \"people\" ][ index ][ \"name\" ] return \" %s is in space\" % ( name ) Deploy the function First, build it: faas-cli build -f ./astronaut-finder.yml Push the function: docker login faas-cli push -f ./astronaut-finder.yml Deploy the function: export OPENFAAS_URL = http://127.0.0.1:31112 cat $HOME /gateway-password.txt | faas-cli login --password-stdin faas-cli deploy -f ./astronaut-finder.yml And now, just wait a bit for the function to be in Ready state. Check from cli with the following command: faas-cli describe astronaut-finder | grep Status You will have a result like this if everythings is up and running: Status: Ready and then try to invoke it from command line: echo | faas-cli invoke astronaut-finder You will receive this as response a random name of a docker user, like this: Anton Shkaplerov is in space You can also use the http endpoint: curl http://localhost:31112/function/astronaut-finder Or try it from the dashboard, just clicking to invoke and see the result in response body : Homework Try to create a function for serving your ML model (you can also make use of: https://github.com/alexellis/tensorflow-serving-openfaas ) Create a function in a different language if you know any","title":"Hands on FaaS triggers"},{"location":"faas/events/#working-with-functions","text":"You can get a local environment ready using Vagrant for an automatically setting up a machine on Virtualbox. To download Vagrant follow this link where you can find the complete list based on Os System: https://www.vagrantup.com/downloads.html","title":"Working with functions"},{"location":"faas/events/#vagrant-installation-windows","text":"Download the following file and install it: https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_x86_64.msi","title":"Vagrant installation - Windows"},{"location":"faas/events/#vagrant-installation-linux","text":"Download the package and extract it just pasting the following commands: wget https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_linux_amd64.zip unzip vagrant_2.2.5_linux_amd64.zip You can also use Homebrew like this: brew cask install vagrant If you prefer, you can use your favorite package manager. For example, in Ubuntu you can type: sudo apt install virtualbox vagrant","title":"Vagrant installation - Linux"},{"location":"faas/events/#vagrant-installation-macos","text":"Download the dmg file and install it: https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_x86_64.dmg If you have Homebrew installed just type this in the command line: brew cask install vagrant","title":"Vagrant installation - MacOS"},{"location":"faas/events/#install-vagrant-project","text":"To be operative with the current project you have to download with git or extract the zip file . Open your command line and paste the following commands: git clone https://github.com/Cloud-PG/SOSC2019.git cd SOSC2019 Now set up the vagrant environment using the following command: ./vagrant up # Or if you have vagrant executable in your PATH vagrant up NOTE : This may take few minutes, depending on network and your computer performance And then, log into the created machine: ./vagrant ssh # Or if you have vagrant executable in your PATH vagrant ssh Remember also to authenticate with OpenFaaS cli before continuing: export OPENFAAS_URL = http://127.0.0.1:31112 cat gateway-password.txt | faas-cli login --password-stdin NOTE : Vagrant and Virtualbox are required on the machine of course. If you don't have them check the previous steps to install the packages Some additional machines have been prepared for the school participants. You can find hot to access to your machine here","title":"Install Vagrant project"},{"location":"faas/events/#using-example-functions-and-the-openfaas-ui","text":"Now you can go to http://localhost:31112/ui/ and, using the password in gateway_password.txt with user admin , you should be able to log in. To see your password just type: cat gateway-password.txt You will have a page like that as result after the login: Let's start playing with some example functions. For instance, you can instantiate a function the face-detection of an online image just clicking on Deploy new function , searching for opencv and installing face-detect with OpenCV (button Deploy ). Now a new tab should appear with the function name selected. From there you can check the status and also try to invoke the function from the UI. For instance, as soon as the status of the function is ready, let's try to put an url with a jpg image in the request body field and then press invoke. Let's try the two below for example: https://parismatch.be/app/uploads/2018/04/Macaca_nigra_self-portrait_large-e1524567086123-1100x715.jpg http://thedreamwithinpictures.com/wp-content/uploads/2013/05/c3a89__montage_2048-copy.jpg For the second one you will have the following result: The list of all available functions in the store is also available from CLI using the following command from the vagrant machine already created: faas-cli store list","title":"Using example functions and the OpenFaaS UI"},{"location":"faas/events/#deployment-of-a-python-function-from-openfaas-workshop","text":"Do everyone have a docker account? mkdir astronaut-finder cd astronaut-finder faas-cli new --lang python3 astronaut-finder --prefix = \"<your-docker-username-here>\"","title":"Deployment of a Python function (from OpenFaaS workshop)"},{"location":"faas/events/#function-fundamentals","text":"The previous command will write three files for us: ./astronaut-finder/handler.py This is the handler for the function. NOTE : an handler get a request object with the raw request and can print the result of the function to the console. ./astronaut-finder/requirements.txt Use this file to list any pip modules you want to install so, to manage your Python requirements, such as requests or urllib ./astronaut-finder.yml This file is used to manage the function: NOTE : it has the name of the function, the Docker image and any other customizations needed. Edit ./astronaut-finder/requirements.txt and add the following dependency: requests","title":"Function fundamentals"},{"location":"faas/events/#write-the-functions-code","text":"We'll be pulling in data from http://api.open-notify.org/astros.json Here's an example of the result we will have from that url: { \"number\" : 6 , \"people\" : [ { \"craft\" : \"ISS\" , \"name\" : \"Alexander Misurkin\" }, { \"craft\" : \"ISS\" , \"name\" : \"Mark Vande Hei\" }, { \"craft\" : \"ISS\" , \"name\" : \"Joe Acaba\" }, { \"craft\" : \"ISS\" , \"name\" : \"Anton Shkaplerov\" }, { \"craft\" : \"ISS\" , \"name\" : \"Scott Tingle\" }, { \"craft\" : \"ISS\" , \"name\" : \"Norishige Kanai\" } ], \"message\" : \"success\" } Let's write an handler that gets for us that result. You have to edit handler.py : import requests import random def handle ( req ): r = requests . get ( \"http://api.open-notify.org/astros.json\" ) result = r . json () index = random . randint ( 0 , result [ \"number\" ] - 1 ) name = result [ \"people\" ][ index ][ \"name\" ] return \" %s is in space\" % ( name )","title":"Write the function's code"},{"location":"faas/events/#deploy-the-function","text":"First, build it: faas-cli build -f ./astronaut-finder.yml Push the function: docker login faas-cli push -f ./astronaut-finder.yml Deploy the function: export OPENFAAS_URL = http://127.0.0.1:31112 cat $HOME /gateway-password.txt | faas-cli login --password-stdin faas-cli deploy -f ./astronaut-finder.yml And now, just wait a bit for the function to be in Ready state. Check from cli with the following command: faas-cli describe astronaut-finder | grep Status You will have a result like this if everythings is up and running: Status: Ready and then try to invoke it from command line: echo | faas-cli invoke astronaut-finder You will receive this as response a random name of a docker user, like this: Anton Shkaplerov is in space You can also use the http endpoint: curl http://localhost:31112/function/astronaut-finder Or try it from the dashboard, just clicking to invoke and see the result in response body :","title":"Deploy the function"},{"location":"faas/events/#homework","text":"Try to create a function for serving your ML model (you can also make use of: https://github.com/alexellis/tensorflow-serving-openfaas ) Create a function in a different language if you know any","title":"Homework"},{"location":"faas/faas/","text":"Set up OpenFaaS OpenFaaS is a functions platform, it stands for \"Functions as a Service\", but can also deploy microservices . With function we mean a small and reusable piece of code that: is short-lived is not a daemon (long-running) is not stateful makes use of your existing services or third-party resources usually executes in a few seconds There is a reach zoology of FaaS frameworks created to provide and to manage functions starting from the one developed by IaaS providers (e.g. AWS Lambda, Google Cloud Functions, Azure functions). There are also many open-source FaaS solutions (as you can see in the figure above) and for this hands-on we will make use of OpenFaaS . Install OpenFaaS on Kubernetes sudo kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml # generate a random password PASSWORD = $( head -c 12 /dev/urandom | shasum | cut -d ' ' -f1 ) sudo kubectl -n openfaas create secret generic basic-auth \\ --from-literal = basic-auth-user = admin \\ --from-literal = basic-auth-password = \" $PASSWORD \" echo $PASSWORD > gateway-password.txt git clone https://github.com/openfaas/faas-netes cd faas-netes && \\ sudo kubectl apply -f ./yaml After few minutes you should be able to go to http://127.0.0.1:31112 to start browsing the OpenFaas WebUI. curl -sL cli.openfaas.com | sudo sh $ faas-cli help $ faas-cli version export OPENFAAS_URL = http://127.0.0.1:31112 echo -n $PASSWORD | faas-cli login --password-stdin Homework Other references https://kubernetes.io/docs/tasks/tools/install-minikube/ https://github.com/kubernetes-sigs/kind https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/ http://kubernetesbyexample.com/","title":"Setup FaaS framework"},{"location":"faas/faas/#set-up-openfaas","text":"OpenFaaS is a functions platform, it stands for \"Functions as a Service\", but can also deploy microservices . With function we mean a small and reusable piece of code that: is short-lived is not a daemon (long-running) is not stateful makes use of your existing services or third-party resources usually executes in a few seconds There is a reach zoology of FaaS frameworks created to provide and to manage functions starting from the one developed by IaaS providers (e.g. AWS Lambda, Google Cloud Functions, Azure functions). There are also many open-source FaaS solutions (as you can see in the figure above) and for this hands-on we will make use of OpenFaaS .","title":"Set up OpenFaaS"},{"location":"faas/faas/#install-openfaas-on-kubernetes","text":"sudo kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml # generate a random password PASSWORD = $( head -c 12 /dev/urandom | shasum | cut -d ' ' -f1 ) sudo kubectl -n openfaas create secret generic basic-auth \\ --from-literal = basic-auth-user = admin \\ --from-literal = basic-auth-password = \" $PASSWORD \" echo $PASSWORD > gateway-password.txt git clone https://github.com/openfaas/faas-netes cd faas-netes && \\ sudo kubectl apply -f ./yaml After few minutes you should be able to go to http://127.0.0.1:31112 to start browsing the OpenFaas WebUI. curl -sL cli.openfaas.com | sudo sh $ faas-cli help $ faas-cli version export OPENFAAS_URL = http://127.0.0.1:31112 echo -n $PASSWORD | faas-cli login --password-stdin","title":"Install OpenFaaS on Kubernetes"},{"location":"faas/faas/#homework","text":"","title":"Homework"},{"location":"faas/faas/#other-references","text":"https://kubernetes.io/docs/tasks/tools/install-minikube/ https://github.com/kubernetes-sigs/kind https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/ http://kubernetesbyexample.com/","title":"Other references"},{"location":"faas/k3s/","text":"Orchestrating containers From Kubernetes overview - \"Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn\u2019t it be easier if this behavior was handled by a system?\" Let's now put our hands on how Kubernetes can help in managing containers. Setup of a local instance In this hands-on we will use K3s for a quick start on your machine but there are plenty of similar solution that you can choose, later below you can find references for some of them. To install it, just execute: curl -sfL https://get.k3s.io | sh - And the verify that the cluster is up and running with: sudo kubectl get nodes k3sup To make installing Kubernetes and building clusters even easier, you can use the k3sup ('ketchup') tool created by OpenFaaS author, Alex Ellis. k3sup installs Kubernetes with k3s via ssh: curl - SLSf https : // get . k3sup . dev / | sudo sh export SERVER = \"IP_ADDRESS_OF_PRIMARY_NODE\" k3sup install -- ip $ SERVER -- username root export SERVER = \"IP_ADDRESS_OF_PRIMARY_NODE\" export AGENT_IP = \"IP_OF_FIRST_AGENT_NODE\" k3sup join -- server - ip $ SERVER -- ip $ AGENT_IP -- username root export KUBECONFIG = `pwd` / kubeconfig kubectl get nodes You can also find many tutorials about k3sup and k3s on the website k3sup.dev Kubernetes fundamentals From Kubernetes docs - \"Kubernetes Objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Specifically, they can describe: What containerized applications are running (and on which nodes) The resources available to those applications The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance A Kubernetes object is a \u201crecord of intent\u201d\u2013once you create the object, the Kubernetes system will constantly work to ensure that object exists. By creating an object, you\u2019re effectively telling the Kubernetes system what you want your cluster\u2019s workload to look like; this is your cluster\u2019s desired state. To work with Kubernetes objects\u2013whether to create, modify, or delete them\u2013you\u2019ll need to use the Kubernetes API. When you use the kubectl command-line interface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use the Kubernetes API directly in your own programs using one of the Client Libraries.\" In this hands-on we will quickly walk through two main objects: Pods and Services. We will use, with some adaptations, Kubernetes by example material, where you can find many other examples for different objects and use cases. Homework Other references https://kubernetes.io/docs/tasks/tools/install-minikube/ https://github.com/kubernetes-sigs/kind https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/ http://kubernetesbyexample.com/","title":"k3s - Deploy kubernetes cluster in 1 min for development"},{"location":"faas/k3s/#orchestrating-containers","text":"From Kubernetes overview - \"Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn\u2019t it be easier if this behavior was handled by a system?\" Let's now put our hands on how Kubernetes can help in managing containers.","title":"Orchestrating containers"},{"location":"faas/k3s/#setup-of-a-local-instance","text":"In this hands-on we will use K3s for a quick start on your machine but there are plenty of similar solution that you can choose, later below you can find references for some of them. To install it, just execute: curl -sfL https://get.k3s.io | sh - And the verify that the cluster is up and running with: sudo kubectl get nodes","title":"Setup of a local instance"},{"location":"faas/k3s/#k3sup","text":"To make installing Kubernetes and building clusters even easier, you can use the k3sup ('ketchup') tool created by OpenFaaS author, Alex Ellis. k3sup installs Kubernetes with k3s via ssh: curl - SLSf https : // get . k3sup . dev / | sudo sh export SERVER = \"IP_ADDRESS_OF_PRIMARY_NODE\" k3sup install -- ip $ SERVER -- username root export SERVER = \"IP_ADDRESS_OF_PRIMARY_NODE\" export AGENT_IP = \"IP_OF_FIRST_AGENT_NODE\" k3sup join -- server - ip $ SERVER -- ip $ AGENT_IP -- username root export KUBECONFIG = `pwd` / kubeconfig kubectl get nodes You can also find many tutorials about k3sup and k3s on the website k3sup.dev","title":"k3sup"},{"location":"faas/k3s/#kubernetes-fundamentals","text":"From Kubernetes docs - \"Kubernetes Objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Specifically, they can describe: What containerized applications are running (and on which nodes) The resources available to those applications The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance A Kubernetes object is a \u201crecord of intent\u201d\u2013once you create the object, the Kubernetes system will constantly work to ensure that object exists. By creating an object, you\u2019re effectively telling the Kubernetes system what you want your cluster\u2019s workload to look like; this is your cluster\u2019s desired state. To work with Kubernetes objects\u2013whether to create, modify, or delete them\u2013you\u2019ll need to use the Kubernetes API. When you use the kubectl command-line interface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use the Kubernetes API directly in your own programs using one of the Client Libraries.\" In this hands-on we will quickly walk through two main objects: Pods and Services. We will use, with some adaptations, Kubernetes by example material, where you can find many other examples for different objects and use cases.","title":"Kubernetes fundamentals"},{"location":"faas/k3s/#homework","text":"","title":"Homework"},{"location":"faas/k3s/#other-references","text":"https://kubernetes.io/docs/tasks/tools/install-minikube/ https://github.com/kubernetes-sigs/kind https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/ http://kubernetesbyexample.com/","title":"Other references"},{"location":"faas/workflows/","text":"Workflows - a.k.a. functions calling functions Building your first workflow - from OpenFaaS workshop Using the CLI to deploy SentimentAnalysis function from the store: mkdir $HOME /workflows cd $HOME /workflows export OPENFAAS_URL = http://127.0.0.1:31112 faas-cli store deploy SentimentAnalysis The Sentiment Analysis function will tell you the subjectivity and polarity (positivity rating) of any sentence. The result of the function is formatted in JSON as you can see with the example below: echo -n \"California is great, it's always sunny there.\" | faas-cli invoke sentimentanalysis # Formatted result { \"polarity\" : 0.8 , \"sentence_count\" : 1 , \"subjectivity\" : 0.75 } Now let's create a new simple function (like in the previous exercise) that will call sentimentanalysis just forwarding the request text. faas-cli new --lang python3 invoker --prefix = \"<your-docker-username-here>\" The handler.py code should look like this: import os import requests import sys def handle ( req ): \"\"\"handle a request to the function Args: req (str): request body \"\"\" gateway_hostname = os . getenv ( \"gateway_hostname\" , \"gateway\" ) test_sentence = req r = requests . get ( \"http://\" + gateway_hostname + \":8080/function/sentimentanalysis\" , data = test_sentence ) if r . status_code != 200 : sys . exit ( \"Error with sentimentanalysis, expected: %d , got: %d \\n \" % ( 200 , r . status_code )) result = r . json () if result [ \"polarity\" ] > 0.45 : return \"That was probably positive\" else : return \"That was neutral or negative\" Put requests in requirements.txt file: echo \"requests\" >> invoker/requirements.txt Remember to set the environment variable gateway_hostname in invoker.yml : version : 1.0 provider : name : openfaas gateway : http://127.0.0.1:31112 functions : invoker : lang : python3 handler : ./invoker image : mircot/invoker:latest environment : gateway_hostname : \"gateway.openfaas\" Then, just deploy our function: faas-cli up -f invoker.yml You can now try to invoke the new function. You can verify that the request has been forwarded to sentimentanalysis by your custom function. We have just created a basic workflow. $ echo -n \"California is bad, it's always rainy there.\" | faas-cli invoke invoker That was neutral or negative $ echo -n \"California is great, it's always sunny there.\" | faas-cli invoke invoker That was probably positive Triggers Example: using storage events webhook If you are using Vagrant image you can start from here, otherwise at the end you'll find how to setup a S3 object storage on your own. Let's configure it properly for using a webhook that points to our openfaas instance (we will use it later to trigger a function as soon as a new file appears). mc admin config set local < /home/vagrant/config_minio.json mc admin service restart local The request sent to the function by Minio in case of a file upload will have a body in this form: { \"EventName\" : \"s3:ObjectCreated:Put\" , \"Key\" : \"images/test7.jpg\" , \"Records\" : [ { \"eventVersion\" : \"2.0\" , \"eventSource\" : \"minio:s3\" , \"awsRegion\" : \"\" , \"eventTime\" : \"2019-09-10T14:27:46Z\" , \"eventName\" : \"s3:ObjectCreated:Put\" , \"userIdentity\" : { \"principalId\" : \"usernameID\" }, \"requestParameters\" : { \"accessKey\" : \"myaccesskey\" , \"region\" : \"\" , \"sourceIPAddress\" : \"192.168.0.213\" }, \"responseElements\" : { \"content-length\" : \"0\" , \"x-amz-request-id\" : \"15C319FC231726B5\" , \"x-minio-deployment-id\" : \"f6a78fdc-8d8e-4d2c-8aca-4b0bd4082129\" , \"x-minio-origin-endpoint\" : \"http://192.168.0.213:9000\" }, \"s3\" : { \"s3SchemaVersion\" : \"1.0\" , \"configurationId\" : \"Config\" , \"bucket\" : { \"name\" : \"images\" , \"ownerIdentity\" : { \"principalId\" : \"usernameID\" }, \"arn\" : \"arn:aws:s3:::images\" }, \"object\" : { \"key\" : \"test7.jpg\" , \"size\" : 1767621 , \"eTag\" : \"1f9ae70259a36b5c1b5692f91386bb75-1\" , \"contentType\" : \"image/jpeg\" , \"userMetadata\" : { \"content-type\" : \"image/jpeg\" }, \"versionId\" : \"1\" , \"sequencer\" : \"15C319FC2679B7CB\" } }, \"source\" : { \"host\" : \"192.168.0.213\" , \"port\" : \"\" , \"userAgent\" : \"MinIO (linux; amd64) minio-go/v6.0.32 mc/2019-09-05T23:43:50Z\" } } ] } Now create two buckets called incoming and processed : mc mb local/incoming mc mb local/processed Set the trigger for any new jpg file appearing in local/incoming: mc event add local/incoming arn:minio:sqs::1:webhook --event put --suffix .jpg You can log into the WebUI at http://localhost:9000/ui/ with username admin and password adminminio . From there you can upload files and check the contents of the buckets. Trigger a facedetect function on loaded images First of all create a new function: mkdir $HOME /triggers cd $HOME /triggers faas-cli new --lang python3 processimage --prefix = \"<your-docker-username-here>\" Then we need to modify the handler to: get the file name from the storage event get the file from the storage encode it in base64 (required as input by the face detection function) call the face detection function get the output and save it back to the storage in a separate bucket A possible result could be: import json from minio import Minio import requests import os import base64 def handle ( st ): \"\"\"handle a request to the function Args: st (str): request body \"\"\" # Decode the json from the Minio event req = json . loads ( st ) # Get configuration parameters from the docker environment (set in the processimage.yml) gateway = os . getenv ( \"openfaas_gw\" , \"gateway.openfaas\" ) # Configure the storage client mc = Minio ( os . environ [ 'minio_hostname' ], access_key = os . environ [ 'minio_access_key' ], secret_key = os . environ [ 'minio_secret_key' ], secure = False ) # Set the name for the source and destination buckets source_bucket = \"incoming\" dest_bucket = \"processed\" # Get the name of the file from the 'Key' field in the event message file_name = req [ 'Key' ] . split ( '/' )[ - 1 ] # Get the file from the storage mc . fget_object ( source_bucket , file_name , \"/tmp/\" + file_name ) # Encode the image into base64 f = open ( \"/tmp/\" + file_name , \"rb\" ) input_image = base64 . b64encode ( f . read ()) # Pass it to the facedetect function r = requests . post ( gateway + \"/function/facedetect\" , input_image ) if r . status_code != 200 : return \"Error during call to facedetect, expected: %d , got: %d \\n \" % ( 200 , r . status_code ) # Finally get the output and save it locally dest_file_name = f \"processed_ {file_name} \" f = open ( \"/tmp/\" + dest_file_name , \"wb\" ) f . write ( r . content ) f . close () f = open ( \"/tmp/input_\" + file_name , \"wb\" ) f . write ( input_image ) f . close () # sync to Minio mc . fput_object ( dest_bucket , dest_file_name , \"/tmp/\" + dest_file_name ) return f \"Image {file_name} processed. Result is in {dest_bucket} \" Now you need to configure the deployment of the functions: version : 1.0 provider : name : openfaas gateway : http://127.0.0.1:31112 functions : # function for loading the image from storage - the code just edited processimage : lang : python3 handler : ./processimage image : <your-docker-username-here>/processimage:latest environment : write_debug : true # environment variables used inside the funcion code minio_hostname : \"10.42.0.1:9000\" minio_access_key : \"admin\" minio_secret_key : \"adminminio\" openfaas_gw : \"http://gateway.openfaas:8080\" # face detection function, pre-built. You can find the source here: # https://github.com/alexellis/facedetect-openfaas facedetect : skip_build : true image : alexellis2/facedetect:0.1 environment : output_mode : \"image\" write_debug : true Before pushing the function in, don't forget to set the requirements.txt: minio requests Then just build and deploy our two functions with: faas-cli build -f processimage.yml faas-cli push -f processimage.yml faas-cli deploy -f processimage.yml Now, once the functions will be ready you should try to upload a .jpg image to the incoming bucket using the WebUI ( login at <your host>:9000 with user admin and password adminminio ) and soon you should be able to find a processed file in the processed bucket that you can download from the webUI and visualize. In the following image you can see an example of the hook result: Homework Create a workflow with 2 functions in different languages Try to create a workflow triggered by a storage event that use the Tensorflow serving function created on the previous set of homeworks EXTRA: Setting up an S3-compatible storage mkdir $HOME /minio_data docker run -d -v $HOME /minio_data:/data --net host -e \"MINIO_ACCESS_KEY=admin\" -e \"MINIO_SECRET_KEY=admindciangot\" minio/minio server /data and the client wget https://dl.min.io/client/mc/release/linux-amd64/mc mv mc /usr/bin/mc sudo chmod +x /usr/bin/mc","title":"Managing storage events and FaaS"},{"location":"faas/workflows/#workflows-aka-functions-calling-functions","text":"","title":"Workflows - a.k.a. functions calling functions"},{"location":"faas/workflows/#building-your-first-workflow-from-openfaas-workshop","text":"Using the CLI to deploy SentimentAnalysis function from the store: mkdir $HOME /workflows cd $HOME /workflows export OPENFAAS_URL = http://127.0.0.1:31112 faas-cli store deploy SentimentAnalysis The Sentiment Analysis function will tell you the subjectivity and polarity (positivity rating) of any sentence. The result of the function is formatted in JSON as you can see with the example below: echo -n \"California is great, it's always sunny there.\" | faas-cli invoke sentimentanalysis # Formatted result { \"polarity\" : 0.8 , \"sentence_count\" : 1 , \"subjectivity\" : 0.75 } Now let's create a new simple function (like in the previous exercise) that will call sentimentanalysis just forwarding the request text. faas-cli new --lang python3 invoker --prefix = \"<your-docker-username-here>\" The handler.py code should look like this: import os import requests import sys def handle ( req ): \"\"\"handle a request to the function Args: req (str): request body \"\"\" gateway_hostname = os . getenv ( \"gateway_hostname\" , \"gateway\" ) test_sentence = req r = requests . get ( \"http://\" + gateway_hostname + \":8080/function/sentimentanalysis\" , data = test_sentence ) if r . status_code != 200 : sys . exit ( \"Error with sentimentanalysis, expected: %d , got: %d \\n \" % ( 200 , r . status_code )) result = r . json () if result [ \"polarity\" ] > 0.45 : return \"That was probably positive\" else : return \"That was neutral or negative\" Put requests in requirements.txt file: echo \"requests\" >> invoker/requirements.txt Remember to set the environment variable gateway_hostname in invoker.yml : version : 1.0 provider : name : openfaas gateway : http://127.0.0.1:31112 functions : invoker : lang : python3 handler : ./invoker image : mircot/invoker:latest environment : gateway_hostname : \"gateway.openfaas\" Then, just deploy our function: faas-cli up -f invoker.yml You can now try to invoke the new function. You can verify that the request has been forwarded to sentimentanalysis by your custom function. We have just created a basic workflow. $ echo -n \"California is bad, it's always rainy there.\" | faas-cli invoke invoker That was neutral or negative $ echo -n \"California is great, it's always sunny there.\" | faas-cli invoke invoker That was probably positive","title":"Building your first workflow - from OpenFaaS workshop"},{"location":"faas/workflows/#triggers","text":"","title":"Triggers"},{"location":"faas/workflows/#example-using-storage-events-webhook","text":"If you are using Vagrant image you can start from here, otherwise at the end you'll find how to setup a S3 object storage on your own. Let's configure it properly for using a webhook that points to our openfaas instance (we will use it later to trigger a function as soon as a new file appears). mc admin config set local < /home/vagrant/config_minio.json mc admin service restart local The request sent to the function by Minio in case of a file upload will have a body in this form: { \"EventName\" : \"s3:ObjectCreated:Put\" , \"Key\" : \"images/test7.jpg\" , \"Records\" : [ { \"eventVersion\" : \"2.0\" , \"eventSource\" : \"minio:s3\" , \"awsRegion\" : \"\" , \"eventTime\" : \"2019-09-10T14:27:46Z\" , \"eventName\" : \"s3:ObjectCreated:Put\" , \"userIdentity\" : { \"principalId\" : \"usernameID\" }, \"requestParameters\" : { \"accessKey\" : \"myaccesskey\" , \"region\" : \"\" , \"sourceIPAddress\" : \"192.168.0.213\" }, \"responseElements\" : { \"content-length\" : \"0\" , \"x-amz-request-id\" : \"15C319FC231726B5\" , \"x-minio-deployment-id\" : \"f6a78fdc-8d8e-4d2c-8aca-4b0bd4082129\" , \"x-minio-origin-endpoint\" : \"http://192.168.0.213:9000\" }, \"s3\" : { \"s3SchemaVersion\" : \"1.0\" , \"configurationId\" : \"Config\" , \"bucket\" : { \"name\" : \"images\" , \"ownerIdentity\" : { \"principalId\" : \"usernameID\" }, \"arn\" : \"arn:aws:s3:::images\" }, \"object\" : { \"key\" : \"test7.jpg\" , \"size\" : 1767621 , \"eTag\" : \"1f9ae70259a36b5c1b5692f91386bb75-1\" , \"contentType\" : \"image/jpeg\" , \"userMetadata\" : { \"content-type\" : \"image/jpeg\" }, \"versionId\" : \"1\" , \"sequencer\" : \"15C319FC2679B7CB\" } }, \"source\" : { \"host\" : \"192.168.0.213\" , \"port\" : \"\" , \"userAgent\" : \"MinIO (linux; amd64) minio-go/v6.0.32 mc/2019-09-05T23:43:50Z\" } } ] } Now create two buckets called incoming and processed : mc mb local/incoming mc mb local/processed Set the trigger for any new jpg file appearing in local/incoming: mc event add local/incoming arn:minio:sqs::1:webhook --event put --suffix .jpg You can log into the WebUI at http://localhost:9000/ui/ with username admin and password adminminio . From there you can upload files and check the contents of the buckets.","title":"Example: using storage events webhook"},{"location":"faas/workflows/#trigger-a-facedetect-function-on-loaded-images","text":"First of all create a new function: mkdir $HOME /triggers cd $HOME /triggers faas-cli new --lang python3 processimage --prefix = \"<your-docker-username-here>\" Then we need to modify the handler to: get the file name from the storage event get the file from the storage encode it in base64 (required as input by the face detection function) call the face detection function get the output and save it back to the storage in a separate bucket A possible result could be: import json from minio import Minio import requests import os import base64 def handle ( st ): \"\"\"handle a request to the function Args: st (str): request body \"\"\" # Decode the json from the Minio event req = json . loads ( st ) # Get configuration parameters from the docker environment (set in the processimage.yml) gateway = os . getenv ( \"openfaas_gw\" , \"gateway.openfaas\" ) # Configure the storage client mc = Minio ( os . environ [ 'minio_hostname' ], access_key = os . environ [ 'minio_access_key' ], secret_key = os . environ [ 'minio_secret_key' ], secure = False ) # Set the name for the source and destination buckets source_bucket = \"incoming\" dest_bucket = \"processed\" # Get the name of the file from the 'Key' field in the event message file_name = req [ 'Key' ] . split ( '/' )[ - 1 ] # Get the file from the storage mc . fget_object ( source_bucket , file_name , \"/tmp/\" + file_name ) # Encode the image into base64 f = open ( \"/tmp/\" + file_name , \"rb\" ) input_image = base64 . b64encode ( f . read ()) # Pass it to the facedetect function r = requests . post ( gateway + \"/function/facedetect\" , input_image ) if r . status_code != 200 : return \"Error during call to facedetect, expected: %d , got: %d \\n \" % ( 200 , r . status_code ) # Finally get the output and save it locally dest_file_name = f \"processed_ {file_name} \" f = open ( \"/tmp/\" + dest_file_name , \"wb\" ) f . write ( r . content ) f . close () f = open ( \"/tmp/input_\" + file_name , \"wb\" ) f . write ( input_image ) f . close () # sync to Minio mc . fput_object ( dest_bucket , dest_file_name , \"/tmp/\" + dest_file_name ) return f \"Image {file_name} processed. Result is in {dest_bucket} \" Now you need to configure the deployment of the functions: version : 1.0 provider : name : openfaas gateway : http://127.0.0.1:31112 functions : # function for loading the image from storage - the code just edited processimage : lang : python3 handler : ./processimage image : <your-docker-username-here>/processimage:latest environment : write_debug : true # environment variables used inside the funcion code minio_hostname : \"10.42.0.1:9000\" minio_access_key : \"admin\" minio_secret_key : \"adminminio\" openfaas_gw : \"http://gateway.openfaas:8080\" # face detection function, pre-built. You can find the source here: # https://github.com/alexellis/facedetect-openfaas facedetect : skip_build : true image : alexellis2/facedetect:0.1 environment : output_mode : \"image\" write_debug : true Before pushing the function in, don't forget to set the requirements.txt: minio requests Then just build and deploy our two functions with: faas-cli build -f processimage.yml faas-cli push -f processimage.yml faas-cli deploy -f processimage.yml Now, once the functions will be ready you should try to upload a .jpg image to the incoming bucket using the WebUI ( login at <your host>:9000 with user admin and password adminminio ) and soon you should be able to find a processed file in the processed bucket that you can download from the webUI and visualize. In the following image you can see an example of the hook result:","title":"Trigger a facedetect function on loaded images"},{"location":"faas/workflows/#homework","text":"Create a workflow with 2 functions in different languages Try to create a workflow triggered by a storage event that use the Tensorflow serving function created on the previous set of homeworks","title":"Homework"},{"location":"faas/workflows/#extra-setting-up-an-s3-compatible-storage","text":"mkdir $HOME /minio_data docker run -d -v $HOME /minio_data:/data --net host -e \"MINIO_ACCESS_KEY=admin\" -e \"MINIO_SECRET_KEY=admindciangot\" minio/minio server /data and the client wget https://dl.min.io/client/mc/release/linux-amd64/mc mv mc /usr/bin/mc sudo chmod +x /usr/bin/mc","title":"EXTRA: Setting up an S3-compatible storage"},{"location":"htcondor/cvmfs/","text":"CVMFS HandsOn Workaround for known bug Docker-XFS: sudo mkdir /opt/cvmfs/spool/ sudo truncate -s 500M /opt/cvmfs/spool/test sudo mkfs.ext4 /opt/cvmfs/spool/test sudo mkdir -p /opt/docker/ sudo mount /opt/cvmfs/spool/test /opt/docker/ Docker compose for deploying a stratum0 + client: version : \"2\" services : cvmfs-client : image : cloudpg/cvmfs-client container_name : cvmfs-client hostname : cvmfs-client privileged : true depends_on : - cvmfs-stratum0 volumes : - /sys/fs/cgroup:/sys/fs/cgroup environment : - \"SERVER_NAME=cvmfs-stratum0\" - \"REPO_NAME=myrepo.dodas\" tty : true cvmfs-stratum0 : image : cloudpg/cvmfs-stratum0-base container_name : cvmfs-stratum0 hostname : cvmfs-stratum0 privileged : true volumes : - /opt/docker/:/var/spool/cvmfs - /var/cvmfs-docker/stratum0/cvmfs:/cvmfs - /var/cvmfs-docker/stratum0/srv/cvmfs:/srv/cvmfs - /var/cvmfs-docker/stratum0/etc/cvmfs:/etc/cvmfs - /sys/fs/cgroup:/sys/fs/cgroup Bring up the containers: docker-compose up Create a cvmfs repository: docker exec -ti cvmfs-stratum0 bash cvmfs_server mkfs myrepo.dodas exit Copy the key of the repository into the client: docker cp /var/cvmfs-docker/stratum0/etc/cvmfs/keys/myrepo.dodas.pub cvmfs-client:/etc/cvmfs/keys docker exec -ti cvmfs-client bash chmod +x /etc/cvmfs-init-scripts/client-init.sh /etc/cvmfs-init-scripts/client-init.sh myrepo.dodas cvmfs-stratum0 ls /cvmfs/myrepo.dodas cat /cvmfs/myrepo.dodas/new_repository exit Log into the Stratum0 and then let's see how to create and publish a file: cvmfs_server transaction myrepo.dodas echo \"testme\" >> /cvmfs/myrepo.dodas/test.txt cvmfs_server publish myrepo.dodas exit Into the client again: # no need to force restart, but not to waste time waiting for periodic update.... /etc/cvmfs-init-scripts/client-init.sh myrepo.dodas cvmfs-stratum0 ls /cvmfs/myrepo.dodas/test.txt cat /cvmfs/myrepo.dodas/test.txt Acknowledgements https://github.com/gabrielefronze/cvmfs-server-container","title":"CVMFS"},{"location":"htcondor/cvmfs/#cvmfs-handson","text":"Workaround for known bug Docker-XFS: sudo mkdir /opt/cvmfs/spool/ sudo truncate -s 500M /opt/cvmfs/spool/test sudo mkfs.ext4 /opt/cvmfs/spool/test sudo mkdir -p /opt/docker/ sudo mount /opt/cvmfs/spool/test /opt/docker/ Docker compose for deploying a stratum0 + client: version : \"2\" services : cvmfs-client : image : cloudpg/cvmfs-client container_name : cvmfs-client hostname : cvmfs-client privileged : true depends_on : - cvmfs-stratum0 volumes : - /sys/fs/cgroup:/sys/fs/cgroup environment : - \"SERVER_NAME=cvmfs-stratum0\" - \"REPO_NAME=myrepo.dodas\" tty : true cvmfs-stratum0 : image : cloudpg/cvmfs-stratum0-base container_name : cvmfs-stratum0 hostname : cvmfs-stratum0 privileged : true volumes : - /opt/docker/:/var/spool/cvmfs - /var/cvmfs-docker/stratum0/cvmfs:/cvmfs - /var/cvmfs-docker/stratum0/srv/cvmfs:/srv/cvmfs - /var/cvmfs-docker/stratum0/etc/cvmfs:/etc/cvmfs - /sys/fs/cgroup:/sys/fs/cgroup Bring up the containers: docker-compose up Create a cvmfs repository: docker exec -ti cvmfs-stratum0 bash cvmfs_server mkfs myrepo.dodas exit Copy the key of the repository into the client: docker cp /var/cvmfs-docker/stratum0/etc/cvmfs/keys/myrepo.dodas.pub cvmfs-client:/etc/cvmfs/keys docker exec -ti cvmfs-client bash chmod +x /etc/cvmfs-init-scripts/client-init.sh /etc/cvmfs-init-scripts/client-init.sh myrepo.dodas cvmfs-stratum0 ls /cvmfs/myrepo.dodas cat /cvmfs/myrepo.dodas/new_repository exit Log into the Stratum0 and then let's see how to create and publish a file: cvmfs_server transaction myrepo.dodas echo \"testme\" >> /cvmfs/myrepo.dodas/test.txt cvmfs_server publish myrepo.dodas exit Into the client again: # no need to force restart, but not to waste time waiting for periodic update.... /etc/cvmfs-init-scripts/client-init.sh myrepo.dodas cvmfs-stratum0 ls /cvmfs/myrepo.dodas/test.txt cat /cvmfs/myrepo.dodas/test.txt","title":"CVMFS HandsOn"},{"location":"htcondor/cvmfs/#acknowledgements","text":"https://github.com/gabrielefronze/cvmfs-server-container","title":"Acknowledgements"},{"location":"htcondor/deployment/","text":"HTCondor on demand Deployment schema AuthN/Z schema Automation flow Deploying HTCondor on demand Retrieve IAM token $HOME /get_proxy.sh You'll be prompted with username and password requests. Just insert the one corresponding to you Indigo-IAM account. Method 1: Deployment with orchestrator Get the templates Template for HTC cluster on demand with orchestrator Quick look at ansible HTCONDOR cluster Ansible Fill the missing parameters .... Deploy After using the $HOME/get_proxy.sh script to retrieve the token, export it into an env variable together with the orchestrator endpoint: export ORCHENT_TOKEN = <your token here> export ORCHENT_URL = https://dodas-paas.cloud.ba.infn.it/orchestrator orchent depcreate condor_orchestrator.yaml '{}' Log into k8s master Get the private key for the master node key: orchent depshow <UUID> Then log in with: chmod 600 mykey ssh -i mykey k8s_master Method 2: Deployment with IM DODAS Get the templates Template for k8s cluster Template for HTC cluster DODAS templates repo Fill the missing parameters .... Validate the template Deploy After using the $HOME/get_proxy.sh script to retrieve the token and to configure the dodas client: dodas create htcondor_k8s-cluster.yaml N.B. the client installation guide and reference can be found here . Log into k8s master Get the private key for the master node key: dodas get vm <infID> 0","title":"Deployment"},{"location":"htcondor/deployment/#htcondor-on-demand","text":"","title":"HTCondor on demand"},{"location":"htcondor/deployment/#deployment-schema","text":"","title":"Deployment schema"},{"location":"htcondor/deployment/#authnz-schema","text":"","title":"AuthN/Z schema"},{"location":"htcondor/deployment/#automation-flow","text":"","title":"Automation flow"},{"location":"htcondor/deployment/#deploying-htcondor-on-demand","text":"","title":"Deploying HTCondor on demand"},{"location":"htcondor/deployment/#retrieve-iam-token","text":"$HOME /get_proxy.sh You'll be prompted with username and password requests. Just insert the one corresponding to you Indigo-IAM account.","title":"Retrieve IAM token"},{"location":"htcondor/deployment/#method-1-deployment-with-orchestrator","text":"","title":"Method 1: Deployment with orchestrator"},{"location":"htcondor/deployment/#get-the-templates","text":"Template for HTC cluster on demand with orchestrator","title":"Get the templates"},{"location":"htcondor/deployment/#quick-look-at-ansible","text":"HTCONDOR cluster Ansible","title":"Quick look at ansible"},{"location":"htcondor/deployment/#fill-the-missing-parameters","text":"....","title":"Fill the missing parameters"},{"location":"htcondor/deployment/#deploy","text":"After using the $HOME/get_proxy.sh script to retrieve the token, export it into an env variable together with the orchestrator endpoint: export ORCHENT_TOKEN = <your token here> export ORCHENT_URL = https://dodas-paas.cloud.ba.infn.it/orchestrator orchent depcreate condor_orchestrator.yaml '{}'","title":"Deploy"},{"location":"htcondor/deployment/#log-into-k8s-master","text":"Get the private key for the master node key: orchent depshow <UUID> Then log in with: chmod 600 mykey ssh -i mykey k8s_master","title":"Log into k8s master"},{"location":"htcondor/deployment/#method-2-deployment-with-im-dodas","text":"","title":"Method 2: Deployment with IM DODAS"},{"location":"htcondor/deployment/#get-the-templates_1","text":"Template for k8s cluster Template for HTC cluster DODAS templates repo","title":"Get the templates"},{"location":"htcondor/deployment/#fill-the-missing-parameters_1","text":"....","title":"Fill the missing parameters"},{"location":"htcondor/deployment/#validate-the-template","text":"","title":"Validate the template"},{"location":"htcondor/deployment/#deploy_1","text":"After using the $HOME/get_proxy.sh script to retrieve the token and to configure the dodas client: dodas create htcondor_k8s-cluster.yaml N.B. the client installation guide and reference can be found here .","title":"Deploy"},{"location":"htcondor/deployment/#log-into-k8s-master_1","text":"Get the private key for the master node key: dodas get vm <infID> 0","title":"Log into k8s master"},{"location":"htcondor/templating/","text":"Deploying HTCondor on demand Templating an HTCondor k8s cluster: Helm Helm helps managing Kubernetes applications through a standard templating. The latest version of Helm is maintained by the CNCF - in collaboration with Microsoft, Google, Bitnami and the Helm contributor community. For this hands on we will use the v2 though, since DODAS is currently in the midle of the migration from v2 to v3. On HelmHub you can find by yourselves the motivation of adopting a widely adopted template format. Helm uses a packaging format called charts. A chart is a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a memcached pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on. Chart folder tree Charts are created as files laid out in a particular directory tree, then they can be packaged into versioned archives to be deployed. wordpress/ Chart.yaml # A YAML file containing information about the chart LICENSE # OPTIONAL: A plain text file containing the license for the chart README.md # OPTIONAL: A human-readable README file values.yaml # The default configuration values for this chart templates/ # A directory of templates that, when combined with values, # will generate valid Kubernetes manifest files. templates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes Values HELM values Templates Ansible role Master/Collector/CCB/negotiator Schedd TTS X509 proxy Worker node Test the deployment To get the compiled K8s manifest from HELM: helm template mychart -x templates/deployment.yaml Live HandsOn: configure condor client condor_status -any EXTRA If you delete the current Helm deployment: $ helm list NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE condor 1 Wed Nov 27 12 :26:18 2019 DEPLOYED helm-chart_htcondor-0.1.0 1 .0 default $ helm delete condor Then you can try by hand: helm install --name mycondor helm-chart_htcondor/ --values /etc/condor_values.yml","title":"Templating"},{"location":"htcondor/templating/#deploying-htcondor-on-demand","text":"","title":"Deploying HTCondor on demand"},{"location":"htcondor/templating/#templating-an-htcondor-k8s-cluster-helm","text":"Helm helps managing Kubernetes applications through a standard templating. The latest version of Helm is maintained by the CNCF - in collaboration with Microsoft, Google, Bitnami and the Helm contributor community. For this hands on we will use the v2 though, since DODAS is currently in the midle of the migration from v2 to v3. On HelmHub you can find by yourselves the motivation of adopting a widely adopted template format. Helm uses a packaging format called charts. A chart is a collection of files that describe a related set of Kubernetes resources. A single chart might be used to deploy something simple, like a memcached pod, or something complex, like a full web app stack with HTTP servers, databases, caches, and so on.","title":"Templating an HTCondor k8s cluster: Helm"},{"location":"htcondor/templating/#chart-folder-tree","text":"Charts are created as files laid out in a particular directory tree, then they can be packaged into versioned archives to be deployed. wordpress/ Chart.yaml # A YAML file containing information about the chart LICENSE # OPTIONAL: A plain text file containing the license for the chart README.md # OPTIONAL: A human-readable README file values.yaml # The default configuration values for this chart templates/ # A directory of templates that, when combined with values, # will generate valid Kubernetes manifest files. templates/NOTES.txt # OPTIONAL: A plain text file containing short usage notes","title":"Chart folder tree"},{"location":"htcondor/templating/#values","text":"HELM values","title":"Values"},{"location":"htcondor/templating/#templates","text":"Ansible role Master/Collector/CCB/negotiator Schedd TTS X509 proxy Worker node","title":"Templates"},{"location":"htcondor/templating/#test-the-deployment","text":"To get the compiled K8s manifest from HELM: helm template mychart -x templates/deployment.yaml Live HandsOn: configure condor client condor_status -any","title":"Test the deployment"},{"location":"htcondor/templating/#extra","text":"If you delete the current Helm deployment: $ helm list NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE condor 1 Wed Nov 27 12 :26:18 2019 DEPLOYED helm-chart_htcondor-0.1.0 1 .0 default $ helm delete condor Then you can try by hand: helm install --name mycondor helm-chart_htcondor/ --values /etc/condor_values.yml","title":"EXTRA"},{"location":"k8s/debugging/","text":"Debugging in Kubernetes Visualize container logs Logging is one option to understand what is going on inside your applications and the cluster at large. Basic logging in Kubernetes makes the output a container produces available, which is a good use case for debugging. More advanced setups consider logs across nodes and store them in a central place, either within the cluster or via a dedicated (cloud-based) service. Kubectl logs kubectl logs --tail=5 logme -c gen kubectl logs -f --since=10s logme -c gen Edit resources kubectl edit deployment ... Extras WebUI Login and tabs https://193.204.89.106:30443/#!/deployment?namespace=form Get your token: $ kubectl describe serviceaccount form<your ID> | grep Tokens Tokens: form<your ID>-token-2fbz $ kubectl describe secret form<your ID>-token-2fbz8 | grep token: token: eyJhbG.......","title":"Debugging"},{"location":"k8s/debugging/#debugging-in-kubernetes","text":"","title":"Debugging in Kubernetes"},{"location":"k8s/debugging/#visualize-container-logs","text":"Logging is one option to understand what is going on inside your applications and the cluster at large. Basic logging in Kubernetes makes the output a container produces available, which is a good use case for debugging. More advanced setups consider logs across nodes and store them in a central place, either within the cluster or via a dedicated (cloud-based) service.","title":"Visualize container logs"},{"location":"k8s/debugging/#kubectl-logs","text":"kubectl logs --tail=5 logme -c gen kubectl logs -f --since=10s logme -c gen","title":"Kubectl logs"},{"location":"k8s/debugging/#edit-resources","text":"kubectl edit deployment ...","title":"Edit resources"},{"location":"k8s/debugging/#extras","text":"","title":"Extras"},{"location":"k8s/debugging/#webui","text":"","title":"WebUI"},{"location":"k8s/debugging/#login-and-tabs","text":"https://193.204.89.106:30443/#!/deployment?namespace=form Get your token: $ kubectl describe serviceaccount form<your ID> | grep Tokens Tokens: form<your ID>-token-2fbz $ kubectl describe secret form<your ID>-token-2fbz8 | grep token: token: eyJhbG.......","title":"Login and tabs"},{"location":"k8s/overview/","text":"Why Kubernetes? The evolution of server setups Kubernetes is an open-source platform that coordinates a highly available cluster of computers that are connected to work as a single unit. Kubernetes automates the distribution and scheduling of application containers across a cluster in a fairly efficient way. A cluster can be deployed on either physical or virtual machines. What will Kubernetes do for you? APIs: Kubernetes is driven by its API. Your CI/CD system can communicate with Kubernetes via its REST and CLI interfaces to carry out actions within the cluster. You can build staging environments. You can automatically deploy new releases. Failure recovering: Kubernetes monitors the containers running within it, performing readiness and health checks via TCP, HTTP, or by running a command within the container. Only healthy resources receive traffic, and those that fail health checks for too long will be restarted by Kubernetes or moved to other nodes in the cluster. Scale automatically. Born with microservices in mind. Declarative model: The Kubernetes model is declarative. You create instructions in YAML that tell it the desired state of the cluster. It then does what it needs to do to maintain this state. Kubernetes is everywhere: You can run it on bare metal servers or in a private cloud. You can run it on the public cloud. You can run applications within a hosted Kubernetes service. You can also create a local dev cluster in few steps with many solutions: k3d minikube Kubernetes architecture in a nutshell The Kubernetes Master is a collection of three processes that run on a single node in your cluster, which is designated as the master node. These processes are: kube-apiserver kube-controller-manager kube-scheduler Each individual Node in your cluster runs two processes: kubelet, which communicates with the Kubernetes Master. kube-proxy, a network proxy which reflects Kubernetes networking services on each node. Moreover, each Node runs a container runtime (like Docker) responsible for pulling the container image from a registry, unpacking the container, and running the application.","title":"Overview"},{"location":"k8s/overview/#why-kubernetes","text":"","title":"Why Kubernetes?"},{"location":"k8s/overview/#the-evolution-of-server-setups","text":"Kubernetes is an open-source platform that coordinates a highly available cluster of computers that are connected to work as a single unit. Kubernetes automates the distribution and scheduling of application containers across a cluster in a fairly efficient way. A cluster can be deployed on either physical or virtual machines.","title":"The evolution of server setups"},{"location":"k8s/overview/#what-will-kubernetes-do-for-you","text":"APIs: Kubernetes is driven by its API. Your CI/CD system can communicate with Kubernetes via its REST and CLI interfaces to carry out actions within the cluster. You can build staging environments. You can automatically deploy new releases. Failure recovering: Kubernetes monitors the containers running within it, performing readiness and health checks via TCP, HTTP, or by running a command within the container. Only healthy resources receive traffic, and those that fail health checks for too long will be restarted by Kubernetes or moved to other nodes in the cluster. Scale automatically. Born with microservices in mind. Declarative model: The Kubernetes model is declarative. You create instructions in YAML that tell it the desired state of the cluster. It then does what it needs to do to maintain this state. Kubernetes is everywhere: You can run it on bare metal servers or in a private cloud. You can run it on the public cloud. You can run applications within a hosted Kubernetes service. You can also create a local dev cluster in few steps with many solutions: k3d minikube","title":"What will Kubernetes do for you?"},{"location":"k8s/overview/#kubernetes-architecture-in-a-nutshell","text":"The Kubernetes Master is a collection of three processes that run on a single node in your cluster, which is designated as the master node. These processes are: kube-apiserver kube-controller-manager kube-scheduler Each individual Node in your cluster runs two processes: kubelet, which communicates with the Kubernetes Master. kube-proxy, a network proxy which reflects Kubernetes networking services on each node. Moreover, each Node runs a container runtime (like Docker) responsible for pulling the container image from a registry, unpacking the container, and running the application.","title":"Kubernetes architecture in  a nutshell"},{"location":"k8s/pods/","text":"Introduction to pods From container to pods Let's see how we can deploy a simple container using kubernetes. To do this we need to introduce two k8s resources: Pod : A pod is a collection of containers sharing a network and mount namespace and is the basic unit of deployment in Kubernetes. All containers in a pod are scheduled on the same node. Deployment : A deployment is a supervisor for pods, giving you fine-grained control over how and when a new pod version is rolled out as well as rolled back to a previous state. Before continuing The docker images used in this demo can be found in: lookup probe Create a pod with kubectl To run a pod all you need is: kubectl run --generator = run-pod/v1 lookup-pod --image = dciangot/lookup --port = 80 If everything went well you can then see your pod running after a while: kubectl get pods NAME READY STATUS RESTARTS AGE lookup-pod 1/1 Running 0 1m Inspect resources Every k8s resource can be inspected with the command describe . This will provide you with a variety of useful information on the state and cofiguration of the resource. For instance you can get the internal pod network IP of the running container in the pod. kubectl describe pod lookup-pod | grep IP: IP: 172 .17.0.3 Delete Delete the two pods and pay attention to what happens. kubectl delete pod lookup-pod Using manifest files Any resource can also be created from a yaml or json template. In case of a pod you can find the configuration file in templates/pods.yml : apiVersion : v1 kind : Pod metadata : name : lookup-manifest-pod spec : containers : - name : lookup-container image : dciangot/lookup ports : - containerPort : 80 resources : limits : memory : \"64Mi\" cpu : \"500m\" As you can see, is also possible to set limits for the resource to be reserved for the pod. Now the syntax for creating a resource from a manifest file is: kubectl create -f templates/k8s/pods.yml And then take a look to the pod details using describe command: kubectl describe pod lookup-manifest-pod And finally remove the pod with: kubectl delete lookup-manifest-pod Create a deployment from configuration file You can find the configuration file for a deployment in templates/deployments.yml : apiVersion : apps/v1beta1 kind : Deployment metadata : name : lookup-deployment spec : replicas : 1 template : metadata : labels : app : myapp spec : containers : - name : lookup-container-deployment image : dciangot/lookup ports : - containerPort : 80 env : - name : SIMPLE_SERVICE_VERSION value : \"1.0\" resources : limits : memory : \"64Mi\" cpu : \"500m\" - name : probe-container image : dciangot/probe Here you can see the addition of the replica field where you can specifiy how many replicas of the same file you want. Also the label metadata is important to organize the application management as we will see next on the k8s service section. kubectl create -f templates/k8s/deployments.yml Also, let's try to see the environment variable set inside the container with the kubectl exec utility. First of all take and note the pod name: kubectl get pod -l app = myapp NAME READY STATUS RESTARTS AGE lookup-deployment-77d5748999-tcbhj 2/2 Running 0 4m11s Then execute env command inside the container with: kubectl exec -ti lookup-deployment-77d5748999-tcbhj env -c lookup-container-deployment | grep SIMPLE_SERVICE_VERSION We can also get to the endpoint from the probe contianer directly on localhost with: $ kubectl exec -ti lookup-deployment-85b549f8b6-4lgls -c probe-container curl localhost <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-4lgls<br/> Update a pod Try to modify the manifest and then run: kubectl apply -f templates/deployments.yml Delete a pod inside a deployment If you try to remove the pod under the deployment control, what happens? Before continuing: can I connect outside the pod? Let's finally create another probe container outside the pod: kubectl run --generator = run-pod/v1 probe-cluster-pod --image = dciangot/probe Run curl in the container and check that indeed you can access the endpoint of the pods at: $ kubectl describe pod lookup-deployment-85b549f8b6-4lgls | grep IP: IP: 10 .244.3.6 $ kubectl exec -ti probe-cluster-pod curl 10 .244.3.6 <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-4lgls<br/>","title":"Pods"},{"location":"k8s/pods/#introduction-to-pods","text":"","title":"Introduction to pods"},{"location":"k8s/pods/#from-container-to-pods","text":"Let's see how we can deploy a simple container using kubernetes. To do this we need to introduce two k8s resources: Pod : A pod is a collection of containers sharing a network and mount namespace and is the basic unit of deployment in Kubernetes. All containers in a pod are scheduled on the same node. Deployment : A deployment is a supervisor for pods, giving you fine-grained control over how and when a new pod version is rolled out as well as rolled back to a previous state.","title":"From container to pods"},{"location":"k8s/pods/#before-continuing","text":"The docker images used in this demo can be found in: lookup probe","title":"Before continuing"},{"location":"k8s/pods/#create-a-pod-with-kubectl","text":"To run a pod all you need is: kubectl run --generator = run-pod/v1 lookup-pod --image = dciangot/lookup --port = 80 If everything went well you can then see your pod running after a while: kubectl get pods NAME READY STATUS RESTARTS AGE lookup-pod 1/1 Running 0 1m","title":"Create a pod with kubectl"},{"location":"k8s/pods/#inspect-resources","text":"Every k8s resource can be inspected with the command describe . This will provide you with a variety of useful information on the state and cofiguration of the resource. For instance you can get the internal pod network IP of the running container in the pod. kubectl describe pod lookup-pod | grep IP: IP: 172 .17.0.3","title":"Inspect resources"},{"location":"k8s/pods/#delete","text":"Delete the two pods and pay attention to what happens. kubectl delete pod lookup-pod","title":"Delete"},{"location":"k8s/pods/#using-manifest-files","text":"Any resource can also be created from a yaml or json template. In case of a pod you can find the configuration file in templates/pods.yml : apiVersion : v1 kind : Pod metadata : name : lookup-manifest-pod spec : containers : - name : lookup-container image : dciangot/lookup ports : - containerPort : 80 resources : limits : memory : \"64Mi\" cpu : \"500m\" As you can see, is also possible to set limits for the resource to be reserved for the pod. Now the syntax for creating a resource from a manifest file is: kubectl create -f templates/k8s/pods.yml And then take a look to the pod details using describe command: kubectl describe pod lookup-manifest-pod And finally remove the pod with: kubectl delete lookup-manifest-pod","title":"Using manifest files"},{"location":"k8s/pods/#create-a-deployment-from-configuration-file","text":"You can find the configuration file for a deployment in templates/deployments.yml : apiVersion : apps/v1beta1 kind : Deployment metadata : name : lookup-deployment spec : replicas : 1 template : metadata : labels : app : myapp spec : containers : - name : lookup-container-deployment image : dciangot/lookup ports : - containerPort : 80 env : - name : SIMPLE_SERVICE_VERSION value : \"1.0\" resources : limits : memory : \"64Mi\" cpu : \"500m\" - name : probe-container image : dciangot/probe Here you can see the addition of the replica field where you can specifiy how many replicas of the same file you want. Also the label metadata is important to organize the application management as we will see next on the k8s service section. kubectl create -f templates/k8s/deployments.yml Also, let's try to see the environment variable set inside the container with the kubectl exec utility. First of all take and note the pod name: kubectl get pod -l app = myapp NAME READY STATUS RESTARTS AGE lookup-deployment-77d5748999-tcbhj 2/2 Running 0 4m11s Then execute env command inside the container with: kubectl exec -ti lookup-deployment-77d5748999-tcbhj env -c lookup-container-deployment | grep SIMPLE_SERVICE_VERSION We can also get to the endpoint from the probe contianer directly on localhost with: $ kubectl exec -ti lookup-deployment-85b549f8b6-4lgls -c probe-container curl localhost <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-4lgls<br/>","title":"Create a deployment from configuration file"},{"location":"k8s/pods/#update-a-pod","text":"Try to modify the manifest and then run: kubectl apply -f templates/deployments.yml","title":"Update a pod"},{"location":"k8s/pods/#delete-a-pod-inside-a-deployment","text":"If you try to remove the pod under the deployment control, what happens?","title":"Delete a pod inside a deployment"},{"location":"k8s/pods/#before-continuing-can-i-connect-outside-the-pod","text":"Let's finally create another probe container outside the pod: kubectl run --generator = run-pod/v1 probe-cluster-pod --image = dciangot/probe Run curl in the container and check that indeed you can access the endpoint of the pods at: $ kubectl describe pod lookup-deployment-85b549f8b6-4lgls | grep IP: IP: 10 .244.3.6 $ kubectl exec -ti probe-cluster-pod curl 10 .244.3.6 <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-4lgls<br/>","title":"Before continuing: can I connect outside the pod?"},{"location":"k8s/services/","text":"Exposing applications in pod We can create a reliable way to access the enpoint inside a pod using the service k8s resource: A service is an abstraction for pods, providing a stable, so called virtual IP (VIP) address. While pods may come and go and with it their IP addresses, a service allows clients to reliably connect to the containers running in the pod using the VIP . The virtual in VIP means it is not an actual IP address connected to a network interface, but its purpose is purely to forward traffic to one or more pods. Keeping the mapping between the VIP and the pods up-to-date is the job of kube-proxy, a process that runs on every node, which queries the API server to learn about new services in the cluster. Type of services ClusterIP : Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType. NodePort : Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting : . LoadBalancer : Exposes the Service externally using a cloud provider\u2019s load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. ExternalName/IPs : Maps the Service to the contents of the externalName/IPs field (e.g. foo.bar.example.com) Create a service Now let's take a look to the yml file for creating a service templates/services.yml : apiVersion : v1 kind : Service metadata : name : simpleservice spec : ports : - port : 300<youID> targetPort : 80 selector : app : myapp To create a service execute: kubectl create -f templates/services.yml Get the cluster IP $ kubectl get svc simpleservice NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE simpleservice ClusterIP 10 .99.69.211 <None> 30102 /TCP 11m Checking connectivity with the cluster probe pod Log into the $ kubectl exec -ti probe-cluster-pod curl 10 .99.69.211:30102 <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-4lgls<br/> Scale Let's scale the lookup-deployment up: kubectl scale deployment lookup-deployment --replicas 3 $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE lookup-deployment 3 /3 3 3 16m $ kubectl get pod NAME READY STATUS RESTARTS AGE lookup-deployment-85b549f8b6-4lgls 2 /2 Running 0 14m lookup-deployment-85b549f8b6-6w4qq 2 /2 Running 0 98s lookup-deployment-85b549f8b6-rhf82 2 /2 Running 0 98s probe-cluster-pod 1 /1 Running 0 9m3s Check the effect of the load-balancing introduced by Kubernetes: $ kubectl exec -ti probe-cluster-pod curl 10 .99.69.211:300<youID> <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-4lgls<br/> $ kubectl exec -ti probe-cluster-pod curl 10 .99.69.211:300<youID> <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-rhf82<br/> kubectl exec -ti probe-cluster-pod curl 10 .99.69.211:300<youID> <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-6w4qq<br/> Extras External IPs apiVersion : v1 kind : Service metadata : name : simpleservice spec : ports : - port : 300<youID> targetPort : 80 externalIPs : - 10.2.201.117 selector : app : myapp curl 10 .2.201.117:300<youID> Ingress Controllers [REF]","title":"Services"},{"location":"k8s/services/#exposing-applications-in-pod","text":"We can create a reliable way to access the enpoint inside a pod using the service k8s resource: A service is an abstraction for pods, providing a stable, so called virtual IP (VIP) address. While pods may come and go and with it their IP addresses, a service allows clients to reliably connect to the containers running in the pod using the VIP . The virtual in VIP means it is not an actual IP address connected to a network interface, but its purpose is purely to forward traffic to one or more pods. Keeping the mapping between the VIP and the pods up-to-date is the job of kube-proxy, a process that runs on every node, which queries the API server to learn about new services in the cluster.","title":"Exposing applications in pod"},{"location":"k8s/services/#type-of-services","text":"ClusterIP : Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType. NodePort : Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting : . LoadBalancer : Exposes the Service externally using a cloud provider\u2019s load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. ExternalName/IPs : Maps the Service to the contents of the externalName/IPs field (e.g. foo.bar.example.com)","title":"Type of services"},{"location":"k8s/services/#create-a-service","text":"Now let's take a look to the yml file for creating a service templates/services.yml : apiVersion : v1 kind : Service metadata : name : simpleservice spec : ports : - port : 300<youID> targetPort : 80 selector : app : myapp To create a service execute: kubectl create -f templates/services.yml","title":"Create a service"},{"location":"k8s/services/#get-the-cluster-ip","text":"$ kubectl get svc simpleservice NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE simpleservice ClusterIP 10 .99.69.211 <None> 30102 /TCP 11m","title":"Get the cluster IP"},{"location":"k8s/services/#checking-connectivity-with-the-cluster-probe-pod","text":"Log into the $ kubectl exec -ti probe-cluster-pod curl 10 .99.69.211:30102 <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-4lgls<br/>","title":"Checking connectivity with the cluster probe pod"},{"location":"k8s/services/#scale","text":"Let's scale the lookup-deployment up: kubectl scale deployment lookup-deployment --replicas 3 $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE lookup-deployment 3 /3 3 3 16m $ kubectl get pod NAME READY STATUS RESTARTS AGE lookup-deployment-85b549f8b6-4lgls 2 /2 Running 0 14m lookup-deployment-85b549f8b6-6w4qq 2 /2 Running 0 98s lookup-deployment-85b549f8b6-rhf82 2 /2 Running 0 98s probe-cluster-pod 1 /1 Running 0 9m3s Check the effect of the load-balancing introduced by Kubernetes: $ kubectl exec -ti probe-cluster-pod curl 10 .99.69.211:300<youID> <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-4lgls<br/> $ kubectl exec -ti probe-cluster-pod curl 10 .99.69.211:300<youID> <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-rhf82<br/> kubectl exec -ti probe-cluster-pod curl 10 .99.69.211:300<youID> <h3>Hello World!</h3><b>Hostname:</b> lookup-deployment-85b549f8b6-6w4qq<br/>","title":"Scale"},{"location":"k8s/services/#extras","text":"","title":"Extras"},{"location":"k8s/services/#external-ips","text":"apiVersion : v1 kind : Service metadata : name : simpleservice spec : ports : - port : 300<youID> targetPort : 80 externalIPs : - 10.2.201.117 selector : app : myapp curl 10 .2.201.117:300<youID>","title":"External IPs"},{"location":"k8s/services/#ingress-controllers","text":"[REF]","title":"Ingress Controllers"}]}